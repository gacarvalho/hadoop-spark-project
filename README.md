## ğŸ“Œ PROPOSTA DO PROJETO

O objetivo do projeto Ã© recuperar um arquivo sobre jogos que estÃ¡ localizado no HDFS com Apache Spark e realizar consultas com o SparkSQL. O arquivo 'DADOS_GAME' foi deslocado da mÃ¡quina local para ficar disponÃ­vel no HDFS (Hadoop Distributed File System): VocÃª pode consultar como foi o processo neste repositÃ³rio - https://github.com/gacarvalho/hadoop-hdfs-project 

ğŸ“¢  ETAPA 1: SERVIÃ‡OS DO HADOOP

O primeiro passo Ã© ativar e verificar como estÃ¡ os serviÃ§os do Hadoop! Os serviÃ§os necessÃ¡rio sÃ£o: NodeManager, ResourceManager, DataNode, Jps, SecondaryNameNode, NameNode. VocÃª pode analisar na imagem abaixo que todos os serviÃ§os estÃ£o ativos.

[Imagem 1]

ğŸ“¢  ETAPA 2: CONSULTANDO O ARQUIVO 'DADOS_GAME' NO HDFS

O segundo passo antes de comeÃ§armos a utilizar o Spark Ã© consultar o conteÃºdo do arquivo DADOS_GAME no HDFS (Hadoop Distributed File System). Como vocÃª pode observar, foi necessÃ¡rio listar os conteÃºdos pelo o comando:

```bash
user@user:/usr/local/hadoop$ bin/hdfs dfs -ls /user/igti/DADOS_GAME
```
Logo apÃ³s foi necessÃ¡rio apresentar o conteÃºdo do arquivo distribuido que estÃ¡ localizado no HDFS! Vale lembrar que o arquivo nÃ£o foi divido em mais parte por conta do seu tamanho. O nÃºmero de registro Ã© de 231! 

```bash
user@user:/usr/local/hadoop$ bin/hdfs dfs -cat /user/igti/DADOS_GAME/part-m-00000
```
[Imagem 2]
[Imagem 2 - 1]

ğŸ“¢  ETAPA 3: COLOCANDO O APACHE SPARK NO MODO ON

Antes de mais nada Ã© necessÃ¡rio se deslocar da pasta do hadoop e ir atÃ© a pasta de instalaÃ§Ã£o do Spark e ativar atraves do comando:

```bash
user@user:/usr/local/spark$ bin/spark-shell
```
[Imagem 3]

ğŸ“¢  ETAPA 4: CRIANDO UM RDD 

Para quem tem conhecimento em Spark, sabe que existe duas formas de criar um RDD (ColeÃ§Ã£o de Dados Imutaveis): (1) Coletando os dados de um sistema de armazenamento externo (2) Aplicando manualmente os valores. Para esse projeto optamos por coletar os dados de um sistema de armazenamento externo, que Ã© o HDFS. Para isso vamos aplicar o caminho do arquivo na ```val dados``` e logo apÃ³s, vamos contar quantos registros existem no documento:

```bash
scala>  val dados = sc.textFile("hdfs://localhost:54310/user/igti/DADOS_GAME/part-m-00000")
```
[Imagem 4]

ApÃ³s essa etapa de recuperar o documento e consultar o nÃºmero de registros, vamos apresentar o conteÃºdo que ```dados``` recebeu pelo comando ```scala> dados.collect()```.

[Imagem 5]

ğŸ“¢  ETAPA 5: DATAFRAME PARA SQL 

Agora vamos criar uma ```val dfGames``` para trabalhar como SQL. Para realizar ess processo Ã© necessÃ¡rio aplicar o cÃ³digo abaixo e depois consultar com o comando ```scala> dfGames.printSchema```

```bash
scala>  val dfGames = sc.textFile("hdfs://localhost:54310/user/igti/DADOS_GAME/part-m-00000")
```
Vale lembrar que o nosso arquivo nÃ£o tem cabeÃ§alho, entÃ£o o Spark tem a capaciade de criar um cabeÃ§alho bÃ¡sico, seguindo a sequencia: 
- [x] _c0 - id
- [x] _c1 - nome_jogo
- [x] _c2 - plataforma
- [x] _c3 - anoLancamento
- [x] _c4 - genero
- [x] _c5 - fabricante
- [x] _c6 - na_sales (Vendas na AmÃ©rica)
- [x] _c7 - eu_sales (Vendas na Europa)
- [x] _c8 - jp_sales (Vendas no JapÃ£o)
- [x] _c9 - other_sales
- [x] _c10 - global_sales

[Imagem 6]

ğŸ“¢  ETAPA 6: CONSULTAS SQL 

Agora vamos executar algumas sentenÃ§as SQL. Mas antes disso, vamos criar uma visÃ£o temporÃ¡ria para que possamos manipular os dados do df com SQL. 

```bash
scala>  dfGames.createOrReplaceTempView("DADOS_GAME")
```
Agora Ã© possÃ­vel consultar os dados pela visÃ£o temporÃ¡ria que foi carregada em 'DADOS_GAME' de forma estruturada.

```bash
scala>  spark.sql("SELECT * FROM DADOS_GAME").show(231)
```
[Imagem 7]

Agora vamos aplicar uma consulta para saber o TOTAL GLOBAL por ANO apenas se for maior do que 10 milhÃµes!

```bash
scala>  spark.sql("SELECT * FROM (SELECT _c3, sum(_c10) as total_global from DADOS_GAME group by _c3 order by _c3) as somador where somador.total_global > 10").show(231)
```
[Imagem 8]

Agora vamos propor uma situaÃ§Ã£o de negÃ³cios! Quanto cada categoria rendeu em milhÃµes? Para isso vamos aplicar a consulta:

```bash
scala>  spark.sql("SELECT _c4, sum(_c10) from DADOS_GAME group by _c4 order by _c4").show(231)
```
[Imagem 9]

E por Ãºltimo, vamos aplicar outra situaÃ§Ã£o de negÃ³cio! Quantos cada fabricante rendeu em milhÃµes entre o ano de 2000 e 2009? Vamos lÃ¡, para isso vamos aplicar a consulta:
```bash
scala>  spark.sql("SELECT _c5 , sum(_c10) from DADOS_GAME where _c3 >= 2000 and _c3 <= 2009 group by _c5").show(231)
```
